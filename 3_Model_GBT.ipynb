{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f7b05ba-a594-4186-a3f0-bb9ac7ab1d9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nClass distribution after balancing:\n+-----+------+\n|label| count|\n+-----+------+\n|  0.0|674488|\n|  1.0|125441|\n+-----+------+\n\n\nConfusion Matrix:\n[[713749    172]\n [ 26693      5]]\n\nClassification Report:\nClass      Precision    Recall  F1-Score   Support\n0             0.9639    0.9998    0.9815    713921\n1             0.0282    0.0002    0.0004     26698\n\nAccuracy                          0.9637    740619\nMacro Avg     0.4961    0.5000    0.4910    740619\nWeighted Avg    0.9302    0.9637    0.9462    740619\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Load feature slicer and prepared data\n",
    "slicer_model = PipelineModel.load(\"/FileStore/models/slicer_top10\")\n",
    "train_ready = spark.read.format(\"delta\").load(\"/FileStore/data/train_ready\")\n",
    "val_ready = spark.read.format(\"delta\").load(\"/FileStore/data/val_ready\")\n",
    "\n",
    "# Apply feature selection\n",
    "train_topk = slicer_model.transform(train_ready)\n",
    "val_topk = slicer_model.transform(val_ready)\n",
    "\n",
    "# Light undersampling of the majority class\n",
    "minority_df = train_topk.filter(col(\"label\") == 1)\n",
    "majority_df = train_topk.filter(col(\"label\") != 1)\n",
    "train_balanced = majority_df.sample(False, 0.2, seed=42).union(minority_df)\n",
    "\n",
    "# Check class distribution after balancing\n",
    "print(\"\\nClass distribution after balancing:\")\n",
    "train_balanced.groupBy(\"label\").count().show()\n",
    "\n",
    "# Define and train the GBT model\n",
    "gbt = GBTClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=20,\n",
    "    maxDepth=5,\n",
    "    seed=42\n",
    ")\n",
    "model = gbt.fit(train_balanced)\n",
    "\n",
    "# Predict on validation set\n",
    "val_preds = model.transform(val_topk)\n",
    "\n",
    "# Apply known threshold\n",
    "def apply_threshold(df, threshold):\n",
    "    predict_udf = udf(lambda prob: float(1.0) if prob[1] > threshold else float(0.0), DoubleType())\n",
    "    return df.withColumn(\"adjusted_prediction\", predict_udf(col(\"probability\")))\n",
    "\n",
    "# Use known best threshold\n",
    "best_threshold = 0.30\n",
    "val_preds_adjusted = apply_threshold(val_preds, best_threshold)\n",
    "\n",
    "# Final evaluation\n",
    "final_rdd = val_preds_adjusted.select(\"adjusted_prediction\", \"label\").rdd.map(lambda r: (float(r[0]), float(r[1])))\n",
    "metrics = MulticlassMetrics(final_rdd)\n",
    "\n",
    "labels = [0.0, 1.0]\n",
    "\n",
    "# Prepare report dictionary\n",
    "report = {}\n",
    "total_support = 0\n",
    "weighted_sum = {\"precision\": 0, \"recall\": 0, \"f1\": 0}\n",
    "\n",
    "for label in labels:\n",
    "    precision = metrics.precision(label)\n",
    "    recall = metrics.recall(label)\n",
    "    f1 = metrics.fMeasure(label)\n",
    "    support = final_rdd.filter(lambda r: r[1] == label).count()\n",
    "    report[label] = {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, \"support\": support}\n",
    "    total_support += support\n",
    "    weighted_sum[\"precision\"] += precision * support\n",
    "    weighted_sum[\"recall\"] += recall * support\n",
    "    weighted_sum[\"f1\"] += f1 * support\n",
    "\n",
    "macro_avg = {\n",
    "    \"precision\": sum(metrics.precision(l) for l in labels) / len(labels),\n",
    "    \"recall\": sum(metrics.recall(l) for l in labels) / len(labels),\n",
    "    \"f1-score\": sum(metrics.fMeasure(l) for l in labels) / len(labels),\n",
    "    \"support\": total_support\n",
    "}\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(metrics.confusionMatrix().toArray().astype(int))\n",
    "\n",
    "# Print classification report like sklearn output\n",
    "print(\"\\nClassification Report:\")\n",
    "print(f\"{'Class':<10}{'Precision':>10}{'Recall':>10}{'F1-Score':>10}{'Support':>10}\")\n",
    "for label in labels:\n",
    "    vals = report[label]\n",
    "    print(f\"{str(int(label)):<10}{vals['precision']:10.4f}{vals['recall']:10.4f}{vals['f1-score']:10.4f}{vals['support']:10d}\")\n",
    "\n",
    "print(f\"\\n{'Accuracy':<10}{'':>10}{'':>10}{metrics.accuracy:10.4f}{total_support:10d}\")\n",
    "print(f\"{'Macro Avg':<10}{macro_avg['precision']:10.4f}{macro_avg['recall']:10.4f}{macro_avg['f1-score']:10.4f}{macro_avg['support']:10d}\")\n",
    "print(f\"{'Weighted Avg':<10}{(weighted_sum['precision']/total_support):10.4f}{(weighted_sum['recall']/total_support):10.4f}{(weighted_sum['f1']/total_support):10.4f}{total_support:10d}\")\n",
    "\n",
    "# Save model\n",
    "model.write().overwrite().save(\"/FileStore/models/gbt_top10_no_weights\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook3_GBT",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}