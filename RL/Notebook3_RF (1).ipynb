{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2b4d0ca-8720-473a-8e33-2551502c7f4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nDistribuiÃ§Ã£o apÃ³s balanceamento:\n+-----+------+\n|label| count|\n+-----+------+\n|  0.0|336896|\n|  1.0|125441|\n+-----+------+\n\n\nðŸ” Threshold Search (para F1 classe 1):\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/context.py:165: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold = 0.05 | F1 (classe 1): 0.0696\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/context.py:165: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold = 0.10 | F1 (classe 1): 0.0696\nThreshold = 0.15 | F1 (classe 1): 0.0696\nThreshold = 0.20 | F1 (classe 1): 0.0696\nThreshold = 0.25 | F1 (classe 1): 0.0696\nThreshold = 0.30 | F1 (classe 1): 0.0696\nThreshold = 0.35 | F1 (classe 1): 0.0696\nThreshold = 0.40 | F1 (classe 1): 0.0696\nThreshold = 0.45 | F1 (classe 1): 0.0764\nThreshold = 0.50 | F1 (classe 1): 0.0817\nThreshold = 0.55 | F1 (classe 1): 0.0000\nThreshold = 0.60 | F1 (classe 1): 0.0000\nThreshold = 0.65 | F1 (classe 1): 0.0000\nThreshold = 0.70 | F1 (classe 1): 0.0000\nThreshold = 0.75 | F1 (classe 1): 0.0000\nThreshold = 0.80 | F1 (classe 1): 0.0000\nThreshold = 0.85 | F1 (classe 1): 0.0000\nThreshold = 0.90 | F1 (classe 1): 0.0000\n\nâœ… Melhor Threshold Encontrado: 0.50 com F1 da classe 1 = 0.0817\n\nConfusion Matrix (com melhor threshold):\n[[395187. 318734.]\n [ 11980.  14718.]]\n\nðŸŽ¯ MÃ©tricas finais com melhor threshold:\nPrecision classe 1: 0.0441\nRecall classe 1:    0.5513\nF1 classe 1:        0.0817\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.ml import PipelineModel\n",
    "# from pyspark.ml.classification import RandomForestClassifier\n",
    "# from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "# from pyspark.sql.functions import col\n",
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.types import DoubleType\n",
    "\n",
    "# # Carregar modelos e dados\n",
    "# slicer_model = PipelineModel.load(\"/FileStore/models/slicer_top10\")\n",
    "# train_ready = spark.read.format(\"delta\").load(\"/FileStore/data/train_ready\")\n",
    "# val_ready = spark.read.format(\"delta\").load(\"/FileStore/data/val_ready\")\n",
    "\n",
    "# # Aplicar slicer\n",
    "# train_topk = slicer_model.transform(train_ready)\n",
    "# val_topk = slicer_model.transform(val_ready)\n",
    "\n",
    "# # Balanceamento leve\n",
    "# minority_df = train_topk.filter(col(\"label\") == 1)\n",
    "# majority_df = train_topk.filter(col(\"label\") != 1)\n",
    "# train_balanced = majority_df.sample(False, 0.1, seed=42).union(minority_df)\n",
    "\n",
    "# # Verificar distribuiÃ§Ã£o apÃ³s balanceamento\n",
    "# print(\"\\nDistribuiÃ§Ã£o apÃ³s balanceamento:\")\n",
    "# train_balanced.groupBy(\"label\").count().show()\n",
    "\n",
    "# # Calcular pesos por classe (class weights)\n",
    "# label_counts = train_balanced.groupBy(\"label\").count().collect()\n",
    "# label_dict = {row[\"label\"]: row[\"count\"] for row in label_counts}\n",
    "# total = sum(label_dict.values())\n",
    "# class_weights = {label: total / count for label, count in label_dict.items()}\n",
    "\n",
    "# # Criar UDF para aplicar pesos\n",
    "# def get_weight(label):\n",
    "#     return float(class_weights[label])\n",
    "\n",
    "# weight_udf = F.udf(get_weight, DoubleType())\n",
    "\n",
    "# # Adicionar coluna classWeightCol\n",
    "# train_weighted = train_balanced.withColumn(\"classWeightCol\", weight_udf(col(\"label\")))\n",
    "\n",
    "# # âž¡ï¸ Substituir GBT por Random Forest\n",
    "# rf = RandomForestClassifier(\n",
    "#     labelCol=\"label\",\n",
    "#     featuresCol=\"features\",\n",
    "#     weightCol=\"classWeightCol\",  # Suporte para weights em RF desde Spark 3.1\n",
    "#     numTrees=100,\n",
    "#     maxDepth=10,\n",
    "#     seed=42\n",
    "# )\n",
    "\n",
    "# # Treinar modelo\n",
    "# model = rf.fit(train_weighted)\n",
    "\n",
    "# # InferÃªncia no conjunto de validaÃ§Ã£o\n",
    "# val_preds = model.transform(val_topk)\n",
    "\n",
    "# # GridSearch de threshold para maximizar F1 da classe 1\n",
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.types import DoubleType\n",
    "\n",
    "# def apply_threshold(df, threshold):\n",
    "#     predict_udf = udf(lambda prob: float(1.0) if prob[1] > threshold else float(0.0), DoubleType())\n",
    "#     return df.withColumn(\"adjusted_prediction\", predict_udf(col(\"probability\")))\n",
    "\n",
    "# best_f1 = 0\n",
    "# best_threshold = 0.5\n",
    "\n",
    "# print(\"\\nðŸ” Threshold Search (para F1 classe 1):\")\n",
    "# for t in [x / 100.0 for x in range(5, 95, 5)]:\n",
    "#     adjusted_df = apply_threshold(val_preds, t)\n",
    "#     rdd = adjusted_df.select(\"adjusted_prediction\", \"label\").rdd.map(lambda r: (float(r[0]), float(r[1])))\n",
    "#     metrics = MulticlassMetrics(rdd)\n",
    "#     f1_class1 = metrics.fMeasure(1.0)\n",
    "#     print(f\"Threshold = {t:.2f} | F1 (classe 1): {f1_class1:.4f}\")\n",
    "#     if f1_class1 > best_f1:\n",
    "#         best_f1 = f1_class1\n",
    "#         best_threshold = t\n",
    "\n",
    "# print(f\"\\nâœ… Melhor Threshold Encontrado: {best_threshold:.2f} com F1 da classe 1 = {best_f1:.4f}\")\n",
    "\n",
    "# # Aplicar melhor threshold\n",
    "# val_preds_adjusted = apply_threshold(val_preds, best_threshold)\n",
    "\n",
    "# # AvaliaÃ§Ã£o final\n",
    "# final_rdd = val_preds_adjusted.select(\"adjusted_prediction\", \"label\").rdd.map(lambda r: (float(r[0]), float(r[1])))\n",
    "# metrics = MulticlassMetrics(final_rdd)\n",
    "\n",
    "# print(\"\\nConfusion Matrix (com melhor threshold):\")\n",
    "# print(metrics.confusionMatrix().toArray())\n",
    "\n",
    "# print(\"\\nðŸŽ¯ MÃ©tricas finais com melhor threshold:\")\n",
    "# print(f\"Precision classe 1: {metrics.precision(1.0):.4f}\")\n",
    "# print(f\"Recall classe 1:    {metrics.recall(1.0):.4f}\")\n",
    "# print(f\"F1 classe 1:        {metrics.fMeasure(1.0):.4f}\")\n",
    "\n",
    "# # Guardar modelo\n",
    "# model.write().overwrite().save(\"/FileStore/models/rf_top10_weighted_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "393cd8a4-1ed2-4a70-a831-bd1894d5d3c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nðŸ“Š Confusion Matrix:\n[[411943. 301978.]\n [ 12738.  13960.]]\n\nðŸŽ¯ MÃ©tricas finais (classe 1):\nPrecision: 0.0442\nRecall:    0.5229\nF1-score:  0.0815\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Carregar slicer e dados\n",
    "slicer_model = PipelineModel.load(\"/FileStore/models/slicer_top10\")\n",
    "train_ready = spark.read.format(\"delta\").load(\"/FileStore/data/train_ready\")\n",
    "val_ready = spark.read.format(\"delta\").load(\"/FileStore/data/val_ready\")\n",
    "\n",
    "# Aplicar feature slicer\n",
    "train_topk = slicer_model.transform(train_ready)\n",
    "val_topk = slicer_model.transform(val_ready)\n",
    "\n",
    "# Balanceamento leve: 40% da classe 0 + 100% da classe 1\n",
    "minority_df = train_topk.filter(col(\"label\") == 1)\n",
    "majority_df = train_topk.filter(col(\"label\") != 1)\n",
    "train_balanced = majority_df.sample(False, 0.4, seed=42).union(minority_df)\n",
    "\n",
    "# Calcular class weights\n",
    "label_counts = train_balanced.groupBy(\"label\").count().collect()\n",
    "label_dict = {row[\"label\"]: row[\"count\"] for row in label_counts}\n",
    "total = sum(label_dict.values())\n",
    "class_weights = {label: total / count for label, count in label_dict.items()}\n",
    "\n",
    "# Adicionar coluna de pesos\n",
    "def get_weight(label):\n",
    "    return float(class_weights[label])\n",
    "\n",
    "weight_udf = udf(get_weight, DoubleType())\n",
    "train_weighted = train_balanced.withColumn(\"classWeightCol\", weight_udf(col(\"label\")))\n",
    "\n",
    "# Treinar Random Forest com pesos\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    weightCol=\"classWeightCol\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "model = rf.fit(train_weighted)\n",
    "\n",
    "# InferÃªncia no conjunto de validaÃ§Ã£o\n",
    "val_preds = model.transform(val_topk)\n",
    "\n",
    "# Aplicar threshold fixo de 0.50\n",
    "predict_udf = udf(lambda prob: float(1.0) if prob[1] > 0.5 else float(0.0), DoubleType())\n",
    "val_preds_adjusted = val_preds.withColumn(\"adjusted_prediction\", predict_udf(col(\"probability\")))\n",
    "\n",
    "# AvaliaÃ§Ã£o com mÃ©tricas\n",
    "rdd = val_preds_adjusted.select(\"adjusted_prediction\", \"label\").rdd.map(lambda r: (float(r[0]), float(r[1])))\n",
    "metrics = MulticlassMetrics(rdd)\n",
    "\n",
    "print(\"\\nðŸ“Š Confusion Matrix:\")\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "\n",
    "print(\"\\nðŸŽ¯ MÃ©tricas finais (classe 1):\")\n",
    "print(f\"Precision: {metrics.precision(1.0):.4f}\")\n",
    "print(f\"Recall:    {metrics.recall(1.0):.4f}\")\n",
    "print(f\"F1-score:  {metrics.fMeasure(1.0):.4f}\")\n",
    "\n",
    "# Guardar o modelo treinado\n",
    "model.write().overwrite().save(\"/FileStore/models/rf_top10_weighted_model_v2\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook3_RF",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}