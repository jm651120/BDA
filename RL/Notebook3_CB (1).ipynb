{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b22023e-baee-44a2-ae28-14c579c1abf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Best Parameters Found: {'depth': 8, 'iterations': 200, 'l2_leaf_reg': 1, 'learning_rate': 0.1}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mCatBoostError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2292840792669070>:96\u001B[0m\n",
       "\u001B[1;32m     91\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m‚úÖ Best Parameters Found:\u001B[39m\u001B[38;5;124m\"\u001B[39m, grid\u001B[38;5;241m.\u001B[39mbest_params_)\n",
       "\u001B[1;32m     93\u001B[0m \u001B[38;5;66;03m# ================================\u001B[39;00m\n",
       "\u001B[1;32m     94\u001B[0m \u001B[38;5;66;03m# 8. Fit with Early Stopping\u001B[39;00m\n",
       "\u001B[1;32m     95\u001B[0m \u001B[38;5;66;03m# ================================\u001B[39;00m\n",
       "\u001B[0;32m---> 96\u001B[0m model\u001B[38;5;241m.\u001B[39mset_params(early_stopping_rounds\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m)\n",
       "\u001B[1;32m     97\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(X_train, y_train, eval_set\u001B[38;5;241m=\u001B[39m(X_val, y_val))\n",
       "\u001B[1;32m     99\u001B[0m \u001B[38;5;66;03m# ================================\u001B[39;00m\n",
       "\u001B[1;32m    100\u001B[0m \u001B[38;5;66;03m# 9. Evaluation\u001B[39;00m\n",
       "\u001B[1;32m    101\u001B[0m \u001B[38;5;66;03m# ================================\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/catboost/core.py:3537\u001B[0m, in \u001B[0;36mCatBoost.set_params\u001B[0;34m(self, **params)\u001B[0m\n",
       "\u001B[1;32m   3528\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   3529\u001B[0m \u001B[38;5;124;03mSet parameters into CatBoost model.\u001B[39;00m\n",
       "\u001B[1;32m   3530\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3534\u001B[0m \u001B[38;5;124;03m    List of key=value paris. Example: model.set_params(iterations=500, thread_count=2).\u001B[39;00m\n",
       "\u001B[1;32m   3535\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   3536\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_fitted():\n",
       "\u001B[0;32m-> 3537\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CatBoostError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou can\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt change params of fitted model.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   3538\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m iteritems(params):\n",
       "\u001B[1;32m   3539\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_params[key] \u001B[38;5;241m=\u001B[39m value\n",
       "\n",
       "\u001B[0;31mCatBoostError\u001B[0m: You can't change params of fitted model."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mCatBoostError\u001B[0m                             Traceback (most recent call last)\nFile \u001B[0;32m<command-2292840792669070>:96\u001B[0m\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m‚úÖ Best Parameters Found:\u001B[39m\u001B[38;5;124m\"\u001B[39m, grid\u001B[38;5;241m.\u001B[39mbest_params_)\n\u001B[1;32m     93\u001B[0m \u001B[38;5;66;03m# ================================\u001B[39;00m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;66;03m# 8. Fit with Early Stopping\u001B[39;00m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;66;03m# ================================\u001B[39;00m\n\u001B[0;32m---> 96\u001B[0m model\u001B[38;5;241m.\u001B[39mset_params(early_stopping_rounds\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m)\n\u001B[1;32m     97\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(X_train, y_train, eval_set\u001B[38;5;241m=\u001B[39m(X_val, y_val))\n\u001B[1;32m     99\u001B[0m \u001B[38;5;66;03m# ================================\u001B[39;00m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;66;03m# 9. Evaluation\u001B[39;00m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;66;03m# ================================\u001B[39;00m\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/catboost/core.py:3537\u001B[0m, in \u001B[0;36mCatBoost.set_params\u001B[0;34m(self, **params)\u001B[0m\n\u001B[1;32m   3528\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3529\u001B[0m \u001B[38;5;124;03mSet parameters into CatBoost model.\u001B[39;00m\n\u001B[1;32m   3530\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3534\u001B[0m \u001B[38;5;124;03m    List of key=value paris. Example: model.set_params(iterations=500, thread_count=2).\u001B[39;00m\n\u001B[1;32m   3535\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3536\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_fitted():\n\u001B[0;32m-> 3537\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CatBoostError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou can\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt change params of fitted model.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   3538\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m iteritems(params):\n\u001B[1;32m   3539\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_params[key] \u001B[38;5;241m=\u001B[39m value\n\n\u001B[0;31mCatBoostError\u001B[0m: You can't change params of fitted model.",
       "errorSummary": "<span class='ansi-red-fg'>CatBoostError</span>: You can't change params of fitted model.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from pyspark.ml import PipelineModel\n",
    "# from pyspark.sql.functions import col, udf\n",
    "# from pyspark.ml.linalg import DenseVector, SparseVector\n",
    "# from pyspark.sql.types import ArrayType, DoubleType\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # ================================\n",
    "# # 1. Load top-k features slicer model\n",
    "# # ================================\n",
    "# slicer_model = PipelineModel.load(\"/FileStore/models/slicer_top10\")\n",
    "\n",
    "# train_ready = spark.read.format(\"delta\").load(\"/FileStore/data/train_ready\")\n",
    "# val_ready = spark.read.format(\"delta\").load(\"/FileStore/data/val_ready\")\n",
    "\n",
    "# # ================================\n",
    "# # 2. Apply slicer to datasets\n",
    "# # ================================\n",
    "# train_topk = slicer_model.transform(train_ready)\n",
    "# val_topk = slicer_model.transform(val_ready)\n",
    "\n",
    "# # ================================\n",
    "# # 3. Light undersampling\n",
    "# # ================================\n",
    "# minority_df = train_topk.filter(col(\"label\") == 1)\n",
    "# majority_df = train_topk.filter(col(\"label\") != 1)\n",
    "# train_balanced = majority_df.sample(False, 0.1, seed=42).union(minority_df)\n",
    "\n",
    "# # ================================\n",
    "# # 4. Convert VectorUDT to list\n",
    "# # ================================\n",
    "# def vector_to_array(v):\n",
    "#     if isinstance(v, DenseVector) or isinstance(v, SparseVector):\n",
    "#         return v.toArray().tolist()\n",
    "#     return v\n",
    "\n",
    "# vector_to_array_udf = udf(vector_to_array, ArrayType(DoubleType()))\n",
    "\n",
    "# train_array = train_balanced.withColumn(\"features_array\", vector_to_array_udf(\"features_topK\"))\n",
    "# val_array = val_topk.withColumn(\"features_array\", vector_to_array_udf(\"features_topK\"))\n",
    "\n",
    "# # ================================\n",
    "# # 5. Convert to Pandas\n",
    "# # ================================\n",
    "# train_pd = train_array.select(\"features_array\", \"label\").toPandas()\n",
    "# val_pd = val_array.select(\"features_array\", \"label\").toPandas()\n",
    "\n",
    "# X_train = pd.DataFrame(train_pd[\"features_array\"].tolist())\n",
    "# y_train = train_pd[\"label\"]\n",
    "\n",
    "# X_val = pd.DataFrame(val_pd[\"features_array\"].tolist())\n",
    "# y_val = val_pd[\"label\"]\n",
    "\n",
    "# # ================================\n",
    "# # 6. Class Weights\n",
    "# # ================================\n",
    "# label_counts = y_train.value_counts().to_dict()\n",
    "# total = sum(label_counts.values())\n",
    "# class_weights = {label: total / count for label, count in label_counts.items()}\n",
    "# class_weights_list = [class_weights[i] for i in sorted(class_weights)]\n",
    "\n",
    "# # ================================\n",
    "# # 7. Hyperparameter Tuning (GridSearch)\n",
    "# # ================================\n",
    "# params = {\n",
    "#     'depth': [4, 6, 8],\n",
    "#     'learning_rate': [0.03, 0.05, 0.1],\n",
    "#     'iterations': [100, 200],\n",
    "#     'l2_leaf_reg': [1, 3, 5],\n",
    "# }\n",
    "\n",
    "# cat = CatBoostClassifier(\n",
    "#     loss_function='MultiClass',\n",
    "#     class_weights=class_weights_list,\n",
    "#     verbose=0,\n",
    "#     random_seed=42\n",
    "# )\n",
    "\n",
    "# grid = GridSearchCV(cat, param_grid=params, cv=3, scoring='f1_macro', n_jobs=-1)\n",
    "# grid.fit(X_train, y_train)\n",
    "\n",
    "# # Best model\n",
    "# model = grid.best_estimator_\n",
    "# print(\"‚úÖ Best Parameters Found:\", grid.best_params_)\n",
    "\n",
    "# # ================================\n",
    "# # 8. Fit with Early Stopping\n",
    "# # ================================\n",
    "# model.set_params(early_stopping_rounds=50, verbose=50)\n",
    "# model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "\n",
    "# # ================================\n",
    "# # 9. Evaluation\n",
    "# # ================================\n",
    "# y_pred = model.predict(X_val)\n",
    "# y_proba = model.predict_proba(X_val)\n",
    "\n",
    "# print(\"\\nüìä Classification Report:\")\n",
    "# print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "# print(\"üìâ Confusion Matrix:\")\n",
    "# print(confusion_matrix(y_val, y_pred))\n",
    "\n",
    "# # AUC por classe\n",
    "# print(\"\\nüéØ AUC Score por classe:\")\n",
    "# for i in range(y_proba.shape[1]):\n",
    "#     auc = roc_auc_score((y_val == i).astype(int), y_proba[:, i])\n",
    "#     print(f\"Classe {i}: AUC = {auc:.4f}\")\n",
    "\n",
    "# # ================================\n",
    "# # 10. Feature Importance Plot\n",
    "# # ================================\n",
    "# feat_imp = model.get_feature_importance()\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# sns.barplot(x=feat_imp, y=[f'feature_{i}' for i in range(len(feat_imp))])\n",
    "# plt.title(\"Feature Importance (Top-10)\")\n",
    "# plt.xlabel(\"Importance\")\n",
    "# plt.ylabel(\"Feature\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # ================================\n",
    "# # 11. Save Model\n",
    "# # ================================\n",
    "# os.makedirs(\"/dbfs/FileStore/models/\", exist_ok=True)\n",
    "# model.save_model(\"/dbfs/FileStore/models/cb_top10_model_tuned.cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6abe4d0c-6b80-427a-8368-b6b90faa6d95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Iniciando grid search para class weights...\n‚úîÔ∏è class_weights=[1, 11] | F1-macro: 0.4909 | Balanced Acc: 0.5000 | Recall-macro: 0.5000 | Pred distrib: Counter({0.0: 740542, 1.0: 77})\n‚úîÔ∏è class_weights=[1, 12] | F1-macro: 0.4913 | Balanced Acc: 0.5001 | Recall-macro: 0.5001 | Pred distrib: Counter({0.0: 740373, 1.0: 246})\n‚úîÔ∏è class_weights=[1, 13] | F1-macro: 0.4917 | Balanced Acc: 0.5002 | Recall-macro: 0.5002 | Pred distrib: Counter({0.0: 740112, 1.0: 507})\n‚úîÔ∏è class_weights=[1, 14] | F1-macro: 0.4942 | Balanced Acc: 0.5007 | Recall-macro: 0.5007 | Pred distrib: Counter({0.0: 738365, 1.0: 2254})\n‚úîÔ∏è class_weights=[1, 15] | F1-macro: 0.4957 | Balanced Acc: 0.5007 | Recall-macro: 0.5007 | Pred distrib: Counter({0.0: 736230, 1.0: 4389})\n‚úîÔ∏è class_weights=[1, 16] | F1-macro: 0.5015 | Balanced Acc: 0.5021 | Recall-macro: 0.5021 | Pred distrib: Counter({0.0: 725974, 1.0: 14645})\n‚úîÔ∏è class_weights=[1, 17] | F1-macro: 0.4864 | Balanced Acc: 0.5159 | Recall-macro: 0.5159 | Pred distrib: Counter({0.0: 627162, 1.0: 113457})\n‚úîÔ∏è class_weights=[1, 18] | F1-macro: 0.3480 | Balanced Acc: 0.5617 | Recall-macro: 0.5617 | Pred distrib: Counter({1.0: 411965, 0.0: 328654})\n‚úîÔ∏è class_weights=[1, 19] | F1-macro: 0.2590 | Balanced Acc: 0.5804 | Recall-macro: 0.5804 | Pred distrib: Counter({1.0: 538331, 0.0: 202288})\n\nüîç Melhor resultado:\nClass weights: [1, 16]\nF1-macro: 0.5015\n\nüìä Classification Report:\n              precision    recall  f1-score   support\n\n         0.0     0.9641    0.9804    0.9722    713921\n         1.0     0.0434    0.0238    0.0308     26698\n\n    accuracy                         0.9459    740619\n   macro avg     0.5038    0.5021    0.5015    740619\nweighted avg     0.9309    0.9459    0.9382    740619\n\nüìâ Confusion Matrix:\n[[699912  14009]\n [ 26062    636]]\n\nüéØ AUC Score por classe:\nClasse 0: AUC = 0.5904\nClasse 1: AUC = 0.5904\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score, balanced_accuracy_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# ================================\n",
    "# 1. Load top-k features slicer model\n",
    "# ================================\n",
    "slicer_model = PipelineModel.load(\"/FileStore/models/slicer_top10\")\n",
    "\n",
    "train_ready = spark.read.format(\"delta\").load(\"/FileStore/data/train_ready\")\n",
    "val_ready = spark.read.format(\"delta\").load(\"/FileStore/data/val_ready\")\n",
    "\n",
    "# ================================\n",
    "# 2. Apply slicer to datasets\n",
    "# ================================\n",
    "train_topk = slicer_model.transform(train_ready)\n",
    "val_topk = slicer_model.transform(val_ready)\n",
    "\n",
    "# ================================\n",
    "# 3. Light undersampling\n",
    "# ================================\n",
    "minority_df = train_topk.filter(col(\"label\") == 1)\n",
    "majority_df = train_topk.filter(col(\"label\") != 1)\n",
    "train_balanced = majority_df.sample(False, 0.8, seed=42).union(minority_df)\n",
    "\n",
    "# ================================\n",
    "# 4. Convert VectorUDT to list\n",
    "# ================================\n",
    "def vector_to_array(v):\n",
    "    if isinstance(v, DenseVector) or isinstance(v, SparseVector):\n",
    "        return v.toArray().tolist()\n",
    "    return v\n",
    "\n",
    "vector_to_array_udf = udf(vector_to_array, ArrayType(DoubleType()))\n",
    "\n",
    "train_array = train_balanced.withColumn(\"features_array\", vector_to_array_udf(\"features_topK\"))\n",
    "val_array = val_topk.withColumn(\"features_array\", vector_to_array_udf(\"features_topK\"))\n",
    "\n",
    "# ================================\n",
    "# 5. Convert to Pandas\n",
    "# ================================\n",
    "train_pd = train_array.select(\"features_array\", \"label\").toPandas()\n",
    "val_pd = val_array.select(\"features_array\", \"label\").toPandas()\n",
    "\n",
    "X_train = pd.DataFrame(train_pd[\"features_array\"].tolist())\n",
    "y_train = train_pd[\"label\"]\n",
    "\n",
    "X_val = pd.DataFrame(val_pd[\"features_array\"].tolist())\n",
    "y_val = val_pd[\"label\"]\n",
    "\n",
    "# ================================\n",
    "# 6‚Äì7. Manual Grid Search for Class Weights\n",
    "# ================================\n",
    "best_score = -1\n",
    "best_weights = None\n",
    "best_model = None\n",
    "\n",
    "weight_options = [11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
    "results = []\n",
    "\n",
    "print(\"üîé Iniciando grid search para class weights...\")\n",
    "\n",
    "for w1 in weight_options:\n",
    "    class_weights_list = [1, w1]  # Classe 0 ‚Üí 1, Classe 1 ‚Üí w1\n",
    "    \n",
    "    model = CatBoostClassifier(\n",
    "        depth=8,\n",
    "        iterations=200,\n",
    "        learning_rate=0.1,\n",
    "        l2_leaf_reg=1,\n",
    "        loss_function='MultiClass',\n",
    "        class_weights=class_weights_list,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=0,\n",
    "        random_seed=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=0)\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred = np.array(y_pred).ravel()  # Garantir array 1D\n",
    "    \n",
    "    unique_preds = np.unique(y_pred)\n",
    "    if len(unique_preds) < 2:\n",
    "        print(f\"‚ö†Ô∏è Ignorado class_weights={class_weights_list} ‚Üí s√≥ previu a classe {unique_preds[0]}\")\n",
    "        continue\n",
    "    \n",
    "    score_f1 = f1_score(y_val, y_pred, average='macro')\n",
    "    score_bal = balanced_accuracy_score(y_val, y_pred)\n",
    "    score_rec = recall_score(y_val, y_pred, average='macro')\n",
    "    \n",
    "    results.append((class_weights_list, score_f1, score_bal, score_rec, model))\n",
    "    \n",
    "    print(f\"‚úîÔ∏è class_weights={class_weights_list} | F1-macro: {score_f1:.4f} | Balanced Acc: {score_bal:.4f} | Recall-macro: {score_rec:.4f} | Pred distrib: {Counter(y_pred)}\")\n",
    "\n",
    "    \n",
    "    if score_f1 > best_score:\n",
    "        best_score = score_f1\n",
    "        best_weights = class_weights_list\n",
    "        best_model = model\n",
    "\n",
    "print(\"\\nüîç Melhor resultado:\")\n",
    "print(f\"Class weights: {best_weights}\")\n",
    "print(f\"F1-macro: {best_score:.4f}\")\n",
    "\n",
    "# ================================\n",
    "# 8. Avalia√ß√£o do melhor modelo\n",
    "# ================================\n",
    "y_pred = best_model.predict(X_val)\n",
    "y_proba = best_model.predict_proba(X_val)\n",
    "\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "print(\"üìâ Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "\n",
    "print(\"\\nüéØ AUC Score por classe:\")\n",
    "for i in range(y_proba.shape[1]):\n",
    "    auc = roc_auc_score((y_val == i).astype(int), y_proba[:, i])\n",
    "    print(f\"Classe {i}: AUC = {auc:.4f}\")\n",
    "\n",
    "# ================================\n",
    "# 9. Save Best Model\n",
    "# ================================\n",
    "os.makedirs(\"/dbfs/FileStore/models/\", exist_ok=True)\n",
    "best_model.save_model(\"/dbfs/FileStore/models/cb_top10_model_best_weights.cbm\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook3_CB",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}