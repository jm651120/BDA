{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2b4d0ca-8720-473a-8e33-2551502c7f4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nThreshold Search (for F1 class 1):\nThreshold = 0.05 | F1 (class 1): 0.0696\nThreshold = 0.10 | F1 (class 1): 0.0696\nThreshold = 0.15 | F1 (class 1): 0.0696\nThreshold = 0.20 | F1 (class 1): 0.0696\nThreshold = 0.25 | F1 (class 1): 0.0696\nThreshold = 0.30 | F1 (class 1): 0.0696\nThreshold = 0.35 | F1 (class 1): 0.0696\nThreshold = 0.40 | F1 (class 1): 0.0696\nThreshold = 0.45 | F1 (class 1): 0.0764\nThreshold = 0.50 | F1 (class 1): 0.0817\nThreshold = 0.55 | F1 (class 1): 0.0000\nThreshold = 0.60 | F1 (class 1): 0.0000\nThreshold = 0.65 | F1 (class 1): 0.0000\nThreshold = 0.70 | F1 (class 1): 0.0000\nThreshold = 0.75 | F1 (class 1): 0.0000\nThreshold = 0.80 | F1 (class 1): 0.0000\nThreshold = 0.85 | F1 (class 1): 0.0000\nThreshold = 0.90 | F1 (class 1): 0.0000\n\nBest Threshold Found: 0.50 with F1 for class 1 = 0.0817\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Load feature slicer model and preprocessed data\n",
    "slicer_model = PipelineModel.load(\"/FileStore/models/slicer_top10\")\n",
    "train_ready = spark.read.format(\"delta\").load(\"/FileStore/data/train_ready\")\n",
    "val_ready = spark.read.format(\"delta\").load(\"/FileStore/data/val_ready\")\n",
    "\n",
    "# Apply feature selection\n",
    "train_topk = slicer_model.transform(train_ready)\n",
    "val_topk = slicer_model.transform(val_ready)\n",
    "\n",
    "# Part 1: Light undersampling of majority class with 10%\n",
    "minority_df = train_topk.filter(col(\"label\") == 1)\n",
    "majority_df = train_topk.filter(col(\"label\") != 1)\n",
    "train_balanced_10 = majority_df.sample(False, 0.1, seed=42).union(minority_df)\n",
    "\n",
    "# Compute class weights\n",
    "label_counts = train_balanced_10.groupBy(\"label\").count().collect()\n",
    "label_dict = {row[\"label\"]: row[\"count\"] for row in label_counts}\n",
    "total = sum(label_dict.values())\n",
    "class_weights = {label: total / count for label, count in label_dict.items()}\n",
    "\n",
    "# Define UDF to assign weights\n",
    "def get_weight(label):\n",
    "    return float(class_weights[label])\n",
    "weight_udf = F.udf(get_weight, DoubleType())\n",
    "\n",
    "# Add class weight column\n",
    "train_weighted_10 = train_balanced_10.withColumn(\"classWeightCol\", weight_udf(col(\"label\")))\n",
    "\n",
    "# Define and train the Random Forest model\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    weightCol=\"classWeightCol\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "model_10 = rf.fit(train_weighted_10)\n",
    "\n",
    "# Predict on validation set\n",
    "val_preds_10 = model_10.transform(val_topk)\n",
    "\n",
    "# Function to apply custom threshold\n",
    "def apply_threshold(df, threshold):\n",
    "    predict_udf = udf(lambda prob: float(1.0) if prob[1] > threshold else float(0.0), DoubleType())\n",
    "    return df.withColumn(\"adjusted_prediction\", predict_udf(col(\"probability\")))\n",
    "\n",
    "# Grid search to find best threshold based on F1-score for class 1\n",
    "best_f1 = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "print(\"\\nThreshold Search (for F1 class 1):\")\n",
    "for t in [x / 100.0 for x in range(5, 95, 5)]:\n",
    "    adjusted_df = apply_threshold(val_preds_10, t)\n",
    "    rdd = adjusted_df.select(\"adjusted_prediction\", \"label\").rdd.map(lambda r: (float(r[0]), float(r[1])))\n",
    "    metrics = MulticlassMetrics(rdd)\n",
    "    f1_class1 = metrics.fMeasure(1.0)\n",
    "    print(f\"Threshold = {t:.2f} | F1 (class 1): {f1_class1:.4f}\")\n",
    "    if f1_class1 > best_f1:\n",
    "        best_f1 = f1_class1\n",
    "        best_threshold = t\n",
    "\n",
    "print(f\"\\nBest Threshold Found: {best_threshold:.2f} with F1 for class 1 = {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e48a5eb-11a6-45eb-8bc1-8153262292c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nConfusion Matrix (40% undersampling):\n[[411943 301978]\n [ 12738  13960]]\n\nClassification Report (40% undersampling):\nClass      Precision    Recall  F1-Score   Support\n0             0.9700    0.5770    0.7236    713921\n1             0.0442    0.5229    0.0815     26698\n\nAccuracy                          0.5751    740619\nMacro Avg     0.5071    0.5500    0.4025    740619\nWeighted Avg    0.9366    0.5751    0.7004    740619\n"
     ]
    }
   ],
   "source": [
    "# Part 2: Use 40% undersampling of majority class with best threshold from Part 1\n",
    "train_balanced_40 = majority_df.sample(False, 0.4, seed=42).union(minority_df)\n",
    "\n",
    "# Compute class weights for 40% sample\n",
    "label_counts_40 = train_balanced_40.groupBy(\"label\").count().collect()\n",
    "label_dict_40 = {row[\"label\"]: row[\"count\"] for row in label_counts_40}\n",
    "total_40 = sum(label_dict_40.values())\n",
    "class_weights_40 = {label: total_40 / count for label, count in label_dict_40.items()}\n",
    "\n",
    "# Define UDF to assign weights for 40% sample\n",
    "def get_weight_40(label):\n",
    "    return float(class_weights_40[label])\n",
    "weight_udf_40 = F.udf(get_weight_40, DoubleType())\n",
    "\n",
    "# Add class weight column\n",
    "train_weighted_40 = train_balanced_40.withColumn(\"classWeightCol\", weight_udf_40(col(\"label\")))\n",
    "\n",
    "# Train new Random Forest model on 40% sample\n",
    "model_40 = rf.fit(train_weighted_40)\n",
    "\n",
    "# Predict on validation set\n",
    "val_preds_40 = model_40.transform(val_topk)\n",
    "\n",
    "# Apply the best threshold found on the new predictions\n",
    "val_preds_adjusted_40 = apply_threshold(val_preds_40, best_threshold)\n",
    "\n",
    "# Final evaluation\n",
    "final_rdd_40 = val_preds_adjusted_40.select(\"adjusted_prediction\", \"label\").rdd.map(lambda r: (float(r[0]), float(r[1])))\n",
    "metrics_40 = MulticlassMetrics(final_rdd_40)\n",
    "\n",
    "labels = [0.0, 1.0]\n",
    "\n",
    "# Prepare report dictionary\n",
    "report_40 = {}\n",
    "total_support_40 = 0\n",
    "weighted_sum_40 = {\"precision\": 0, \"recall\": 0, \"f1\": 0}\n",
    "\n",
    "for label in labels:\n",
    "    precision = metrics_40.precision(label)\n",
    "    recall = metrics_40.recall(label)\n",
    "    f1 = metrics_40.fMeasure(label)\n",
    "    support = final_rdd_40.filter(lambda r: r[1] == label).count()\n",
    "    report_40[label] = {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, \"support\": support}\n",
    "    total_support_40 += support\n",
    "    weighted_sum_40[\"precision\"] += precision * support\n",
    "    weighted_sum_40[\"recall\"] += recall * support\n",
    "    weighted_sum_40[\"f1\"] += f1 * support\n",
    "\n",
    "macro_avg_40 = {\n",
    "    \"precision\": sum(metrics_40.precision(l) for l in labels) / len(labels),\n",
    "    \"recall\": sum(metrics_40.recall(l) for l in labels) / len(labels),\n",
    "    \"f1-score\": sum(metrics_40.fMeasure(l) for l in labels) / len(labels),\n",
    "    \"support\": total_support_40\n",
    "}\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix (40% undersampling):\")\n",
    "print(metrics_40.confusionMatrix().toArray().astype(int))\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report (40% undersampling):\")\n",
    "print(f\"{'Class':<10}{'Precision':>10}{'Recall':>10}{'F1-Score':>10}{'Support':>10}\")\n",
    "for label in labels:\n",
    "    vals = report_40[label]\n",
    "    print(f\"{str(int(label)):<10}{vals['precision']:10.4f}{vals['recall']:10.4f}{vals['f1-score']:10.4f}{vals['support']:10d}\")\n",
    "\n",
    "accuracy_40 = final_rdd_40.filter(lambda x: x[0] == x[1]).count() / final_rdd_40.count()\n",
    "print(f\"\\n{'Accuracy':<10}{'':>10}{'':>10}{accuracy_40:10.4f}{total_support_40:10d}\")\n",
    "print(f\"{'Macro Avg':<10}{macro_avg_40['precision']:10.4f}{macro_avg_40['recall']:10.4f}{macro_avg_40['f1-score']:10.4f}{macro_avg_40['support']:10d}\")\n",
    "print(f\"{'Weighted Avg':<10}{(weighted_sum_40['precision']/total_support_40):10.4f}{(weighted_sum_40['recall']/total_support_40):10.4f}{(weighted_sum_40['f1']/total_support_40):10.4f}{total_support_40:10d}\")\n",
    "\n",
    "# Save model trained on 40% sample\n",
    "model_40.write().overwrite().save(\"/FileStore/models/rf_top10_weighted_model_40pct\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook3_RF",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}