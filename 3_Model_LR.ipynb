{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7df8ae09-6f94-4dd9-ad7b-21d10b51190a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nLabel distribution after balancing:\n+-----+-------+\n|label|  count|\n+-----+-------+\n|  0.0|2699450|\n|  1.0| 125441|\n+-----+-------+\n\n\n Threshold Search (for F1-score of class 1):\nThreshold = 0.05 | F1 (class 1): 0.0696\nThreshold = 0.10 | F1 (class 1): 0.0696\nThreshold = 0.15 | F1 (class 1): 0.0696\nThreshold = 0.20 | F1 (class 1): 0.0696\nThreshold = 0.25 | F1 (class 1): 0.0696\nThreshold = 0.30 | F1 (class 1): 0.0696\nThreshold = 0.35 | F1 (class 1): 0.0696\nThreshold = 0.40 | F1 (class 1): 0.0696\nThreshold = 0.45 | F1 (class 1): 0.0702\nThreshold = 0.50 | F1 (class 1): 0.0727\nThreshold = 0.55 | F1 (class 1): 0.0645\nThreshold = 0.60 | F1 (class 1): 0.0282\nThreshold = 0.65 | F1 (class 1): 0.0095\nThreshold = 0.70 | F1 (class 1): 0.0025\nThreshold = 0.75 | F1 (class 1): 0.0004\nThreshold = 0.80 | F1 (class 1): 0.0001\nThreshold = 0.85 | F1 (class 1): 0.0000\nThreshold = 0.90 | F1 (class 1): 0.0000\n\n Best Threshold Found: 0.50 with F1-score (class 1) = 0.0727\n\nConfusion Matrix:\n[[371688 342233]\n [ 12772  13926]]\n\nClassification Report:\nClass      Precision    Recall  F1-Score   Support\n0             0.9668    0.5206    0.6768    713921\n1             0.0391    0.5216    0.0727     26698\n\nAccuracy                          0.5207    740619\nMacro Avg     0.5029    0.5211    0.3748    740619\nWeighted Avg    0.9333    0.5207    0.6550    740619\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Load models and data\n",
    "slicer_model = PipelineModel.load(\"/FileStore/models/slicer_top10\")\n",
    "train_ready = spark.read.format(\"delta\").load(\"/FileStore/data/train_ready\")\n",
    "val_ready = spark.read.format(\"delta\").load(\"/FileStore/data/val_ready\")\n",
    "\n",
    "# Apply feature slicer\n",
    "train_topk = slicer_model.transform(train_ready)\n",
    "val_topk = slicer_model.transform(val_ready)\n",
    "\n",
    "# Apply light undersampling to balance the training set\n",
    "minority_df = train_topk.filter(col(\"label\") == 1)\n",
    "majority_df = train_topk.filter(col(\"label\") != 1)\n",
    "train_balanced = majority_df.sample(False, 0.8, seed=42).union(minority_df)\n",
    "\n",
    "# Check label distribution after balancing\n",
    "print(\"\\nLabel distribution after balancing:\")\n",
    "train_balanced.groupBy(\"label\").count().show()\n",
    "\n",
    "# Compute class weights\n",
    "label_counts = train_balanced.groupBy(\"label\").count().collect()\n",
    "label_dict = {row[\"label\"]: row[\"count\"] for row in label_counts}\n",
    "total = sum(label_dict.values())\n",
    "class_weights = {label: total / count for label, count in label_dict.items()}\n",
    "\n",
    "# Create UDF to assign weights\n",
    "def get_weight(label):\n",
    "    return float(class_weights[label])\n",
    "\n",
    "weight_udf = F.udf(get_weight, DoubleType())\n",
    "\n",
    "# Add class weight column\n",
    "train_weighted = train_balanced.withColumn(\"classWeightCol\", weight_udf(col(\"label\")))\n",
    "\n",
    "# Define Logistic Regression model\n",
    "lr = LogisticRegression(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    weightCol=\"classWeightCol\",\n",
    "    maxIter=100,\n",
    "    regParam=0.01,\n",
    "    elasticNetParam=0.0  # Ridge regularization\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model = lr.fit(train_weighted)\n",
    "\n",
    "# Perform inference on validation set\n",
    "val_preds = model.transform(val_topk)\n",
    "\n",
    "# Define function to apply a custom threshold\n",
    "def apply_threshold(df, threshold):\n",
    "    predict_udf = udf(lambda prob: float(1.0) if prob[1] > threshold else float(0.0), DoubleType())\n",
    "    return df.withColumn(\"adjusted_prediction\", predict_udf(col(\"probability\")))\n",
    "\n",
    "# Grid search for best threshold based on F1-score for class 1\n",
    "best_f1 = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "print(\"\\n Threshold Search (for F1-score of class 1):\")\n",
    "for t in [x / 100.0 for x in range(5, 95, 5)]:\n",
    "    adjusted_df = apply_threshold(val_preds, t)\n",
    "    rdd = adjusted_df.select(\"adjusted_prediction\", \"label\").rdd.map(lambda r: (float(r[0]), float(r[1])))\n",
    "    metrics = MulticlassMetrics(rdd)\n",
    "    f1_class1 = metrics.fMeasure(1.0)\n",
    "    print(f\"Threshold = {t:.2f} | F1 (class 1): {f1_class1:.4f}\")\n",
    "    if f1_class1 > best_f1:\n",
    "        best_f1 = f1_class1\n",
    "        best_threshold = t\n",
    "\n",
    "print(f\"\\n Best Threshold Found: {best_threshold:.2f} with F1-score (class 1) = {best_f1:.4f}\")\n",
    "\n",
    "# Apply the best threshold\n",
    "val_preds_adjusted = apply_threshold(val_preds, best_threshold)\n",
    "\n",
    "# Final evaluation\n",
    "final_rdd = val_preds_adjusted.select(\"adjusted_prediction\", \"label\").rdd.map(lambda r: (float(r[0]), float(r[1])))\n",
    "metrics = MulticlassMetrics(final_rdd)\n",
    "\n",
    "labels = [0.0, 1.0]\n",
    "\n",
    "# Prepare report dictionary\n",
    "report = {}\n",
    "total_support = 0\n",
    "weighted_sum = {\"precision\": 0, \"recall\": 0, \"f1\": 0}\n",
    "\n",
    "for label in labels:\n",
    "    precision = metrics.precision(label)\n",
    "    recall = metrics.recall(label)\n",
    "    f1 = metrics.fMeasure(label)\n",
    "    support = final_rdd.filter(lambda r: r[1] == label).count()\n",
    "    report[label] = {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, \"support\": support}\n",
    "    total_support += support\n",
    "    weighted_sum[\"precision\"] += precision * support\n",
    "    weighted_sum[\"recall\"] += recall * support\n",
    "    weighted_sum[\"f1\"] += f1 * support\n",
    "\n",
    "macro_avg = {\n",
    "    \"precision\": sum(metrics.precision(l) for l in labels) / len(labels),\n",
    "    \"recall\": sum(metrics.recall(l) for l in labels) / len(labels),\n",
    "    \"f1-score\": sum(metrics.fMeasure(l) for l in labels) / len(labels),\n",
    "    \"support\": total_support\n",
    "}\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(metrics.confusionMatrix().toArray().astype(int))\n",
    "\n",
    "# Print classification report like sklearn output\n",
    "print(\"\\nClassification Report:\")\n",
    "print(f\"{'Class':<10}{'Precision':>10}{'Recall':>10}{'F1-Score':>10}{'Support':>10}\")\n",
    "for label in labels:\n",
    "    vals = report[label]\n",
    "    print(f\"{str(int(label)):<10}{vals['precision']:10.4f}{vals['recall']:10.4f}{vals['f1-score']:10.4f}{vals['support']:10d}\")\n",
    "\n",
    "print(f\"\\n{'Accuracy':<10}{'':>10}{'':>10}{metrics.accuracy:10.4f}{total_support:10d}\")\n",
    "print(f\"{'Macro Avg':<10}{macro_avg['precision']:10.4f}{macro_avg['recall']:10.4f}{macro_avg['f1-score']:10.4f}{macro_avg['support']:10d}\")\n",
    "print(f\"{'Weighted Avg':<10}{(weighted_sum['precision']/total_support):10.4f}{(weighted_sum['recall']/total_support):10.4f}{(weighted_sum['f1']/total_support):10.4f}{total_support:10d}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.write().overwrite().save(\"/FileStore/models/lr_top10_weighted_model\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook3_LR",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}