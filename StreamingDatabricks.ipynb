{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1bbd7b3-3d4b-4256-9c1e-1732e71b8c4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# STREAMING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1e2d510-2dc4-4eee-a562-f36d0d7084c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Read kafka stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c171c3c6-6693-4dcf-a959-78183e21786e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import (StructType, StructField, StringType, DoubleType,\n",
    "                               IntegerType, BooleanType, TimestampType)\n",
    "\n",
    "# Schema that mirrors the data being sent to kafka\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"sender_account\", StringType(), True),\n",
    "    StructField(\"receiver_account\", StringType(), True),\n",
    "    StructField(\"amount\", StringType(), True),\n",
    "    StructField(\"transaction_type\", StringType(), True),\n",
    "    StructField(\"merchant_category\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"device_used\", StringType(), True),\n",
    "    StructField(\"is_fraud\", StringType(), True),\n",
    "    StructField(\"time_since_last_transaction\", StringType(), True),\n",
    "    StructField(\"spending_deviation_score\", StringType(), True),\n",
    "    StructField(\"velocity_score\", StringType(), True),\n",
    "    StructField(\"geo_anomaly_score\", StringType(), True),\n",
    "    StructField(\"payment_channel\", StringType(), True),\n",
    "    StructField(\"ip_address\", StringType(), True),\n",
    "    StructField(\"device_hash\", StringType(), True),\n",
    "    StructField(\"event_time\", StringType(), True),\n",
    "    StructField(\"time_since_last_tx_tmp\", StringType(), True),\n",
    "    StructField(\"time_since_last_transaction_imp\", StringType(), True),\n",
    "    StructField(\"hour_of_day\", StringType(), True),\n",
    "    StructField(\"day_of_week\", StringType(), True),\n",
    "    StructField(\"is_weekend\", StringType(), False),\n",
    "    StructField(\"month\", StringType(), True),\n",
    "    StructField(\"txns_1h\", StringType(), False),\n",
    "    StructField(\"amt_24h\", StringType(), False),\n",
    "    StructField(\"uniq_rcv_7d\", StringType(), False),\n",
    "    StructField(\"avg_gap_mins_10\", StringType(), False),\n",
    "])\n",
    "dbutils.fs.rm(\"dbfs:/tmp/stream\", recurse=True)\n",
    "spark.conf.set(\"spark.sql.streaming.checkpointLocation\", \"/tmp/stream\") #set scheck point\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f88d99d9-2246-445a-90c6-4640cdc94867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Read from Kafka\n",
    "raw_df = (spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafkainterface.ddns.net:9092\")\n",
    "    .option(\"subscribe\", \"streaming\")\n",
    "    .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a84b9a42-7e5a-40b2-973c-d10be01b2f41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Parse JSON → struct → columns\n",
    "parsed_df = (raw_df\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"))\n",
    "    .select(\"data.*\"))          # format raw_df to fit the schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "883df5e0-cdce-40f3-9537-c20a4ffe82f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "# cast the correct datatypes\n",
    "casted_df = (\n",
    "    parsed_df\n",
    "        # ────────── timestamps ──────────\n",
    "        .withColumn(\"timestamp\",   to_timestamp(\"timestamp\"))\n",
    "        .withColumn(\"event_time\",  to_timestamp(\"event_time\"))\n",
    "\n",
    "        # ────────── doubles ──────────\n",
    "        .withColumn(\"amount\",                      col(\"amount\").cast(\"double\"))\n",
    "        .withColumn(\"time_since_last_transaction\", col(\"time_since_last_transaction\").cast(\"double\"))\n",
    "        .withColumn(\"spending_deviation_score\",    col(\"spending_deviation_score\").cast(\"double\"))\n",
    "        .withColumn(\"velocity_score\",              col(\"velocity_score\").cast(\"double\"))\n",
    "        .withColumn(\"geo_anomaly_score\",           col(\"geo_anomaly_score\").cast(\"double\"))\n",
    "        .withColumn(\"time_since_last_tx_tmp\",      col(\"time_since_last_tx_tmp\").cast(\"double\"))\n",
    "        .withColumn(\"time_since_last_transaction_imp\", col(\"time_since_last_transaction_imp\").cast(\"double\"))\n",
    "        .withColumn(\"amt_24h\",                     col(\"amt_24h\").cast(\"double\"))\n",
    "        .withColumn(\"avg_gap_mins_10\",             col(\"avg_gap_mins_10\").cast(\"double\"))\n",
    "        # ────────── integers ──────────\n",
    "        .withColumn(\"hour_of_day\", col(\"hour_of_day\").cast(\"int\"))\n",
    "        .withColumn(\"day_of_week\", col(\"day_of_week\").cast(\"int\"))\n",
    "        .withColumn(\"is_weekend\",  col(\"is_weekend\").cast(\"int\"))\n",
    "        .withColumn(\"month\",       col(\"month\").cast(\"int\"))\n",
    "\n",
    "        # ────────── longs ──────────\n",
    "        .withColumn(\"txns_1h\",     col(\"txns_1h\").cast(\"long\"))\n",
    "        .withColumn(\"uniq_rcv_7d\", col(\"uniq_rcv_7d\").cast(\"long\"))\n",
    "\n",
    "        # ────────── booleans ──────────\n",
    "        .drop(\"is_fraud\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8140cc43-5064-4f12-b506-b86239d66ce8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write casted_df(stream) into a delta dataframe for\n",
    "query = casted_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/stream/Streaming_data\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .start(\"/mnt/datalake/Streaming_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf93af00-a143-4f5b-a6c7-56f0a9e890d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- transaction_id: string (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n |-- sender_account: string (nullable = true)\n |-- receiver_account: string (nullable = true)\n |-- amount: double (nullable = true)\n |-- transaction_type: string (nullable = true)\n |-- merchant_category: string (nullable = true)\n |-- location: string (nullable = true)\n |-- device_used: string (nullable = true)\n |-- time_since_last_transaction: double (nullable = true)\n |-- spending_deviation_score: double (nullable = true)\n |-- velocity_score: double (nullable = true)\n |-- geo_anomaly_score: double (nullable = true)\n |-- payment_channel: string (nullable = true)\n |-- ip_address: string (nullable = true)\n |-- device_hash: string (nullable = true)\n |-- event_time: timestamp (nullable = true)\n |-- time_since_last_tx_tmp: double (nullable = true)\n |-- time_since_last_transaction_imp: double (nullable = true)\n |-- hour_of_day: integer (nullable = true)\n |-- day_of_week: integer (nullable = true)\n |-- is_weekend: integer (nullable = true)\n |-- month: integer (nullable = true)\n |-- txns_1h: long (nullable = true)\n |-- amt_24h: double (nullable = true)\n |-- uniq_rcv_7d: long (nullable = true)\n |-- avg_gap_mins_10: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "#make sure the dataset is ready to be feed into the preprocessing pipelines\n",
    "casted_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d663db8-676a-47f7-b67a-3452eaf67e9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Load Preprocessing Pipeline and Slicer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84038c9c-307a-4313-ba60-bd8271c3f659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "preproc_model = PipelineModel.load(\"/FileStore/models/preproc_model\")\n",
    "slicer_model = PipelineModel.load(\"/FileStore/models/slicer_top10\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a60998c1-0853-4e3f-b40d-6a91f19369ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#transform the data to be fed into the model\n",
    "sliceed_df = slicer_model.transform(preproc_model.transform(casted_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c41d6818-5dd3-4c32-bfa4-9bbdd46e1c78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Load Model and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de08d60f-2066-4d11-9833-405c5e24b708",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "model_path = \"/FileStore/models/rf_top10_weighted_model_40pct\"\n",
    "model = RandomForestClassificationModel.load(model_path)\n",
    "\n",
    "# add probability + prediction columns\n",
    "scored_df = model.transform(sliceed_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fb3272d-83ed-4377-ada5-e215a60b8ca7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Write data with model prediction to kafka "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "441b06ee-317e-4ce1-84e4-378afe5fc204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#stream results back to kafka\n",
    "# make a memory sink(in order to query the df in databricks)\n",
    "out = (\n",
    "    scored_df\n",
    "      .selectExpr(\n",
    "          \"CAST(transaction_id AS STRING) AS key\",\n",
    "          \"to_json(struct(*))          AS value\"     # every column → one JSON blob\n",
    "      )\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2.  Stream it back to Kafka\n",
    "# -------------------------------------------------------------------\n",
    "kafka_query = (\n",
    "    out.writeStream\n",
    "       .format(\"kafka\")\n",
    "       .option(\"kafka.bootstrap.servers\", \"kafkainterface.ddns.net:9092\")\n",
    "       .option(\"topic\", \"scored-transactions\")               # create this topic first\n",
    "       .option(\"checkpointLocation\", \"dbfs:/checkpoints/scored_df→kafka\")\n",
    "       .outputMode(\"append\")                                 # required for Kafka sink\n",
    "       .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afc28ee8-3728-427f-a863-2f8155905879",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "The model prediction and data are available on another kafka topic which streamlines production and makes the building of the dashboard easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9358588d-95be-40fa-b330-3fc85ea0e73c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4050770928218379,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "kafka",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}